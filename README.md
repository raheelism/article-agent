# Article Agent

<p align="center">
  <strong>ü§ñ AI-Powered Article Generation System</strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/AI%20Detection-0%25-brightgreen?style=for-the-badge" alt="0% AI Detection" />
  <img src="https://img.shields.io/badge/Human--Like%20Output-100%25-blue?style=for-the-badge" alt="100% Human-Like" />
</p>

A sophisticated "Deep Agent" system for generating high-quality, SEO-optimized articles using **LangGraph**, **FastAPI**, and **Groq** (Llama 4 / GPT OSS / Qwen / Kimi). The system employs a multi-agent architecture with autonomous research, structured writing, and parallel evaluation capabilities.

> **üéØ Key Achievement:** Generated articles pass AI detection tools with **0% AI-detected content**, thanks to the advanced Humanization Loop with reflexion-based rewriting and anti-AI writing constraints.

---

## üìã Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [API Reference](#-api-reference)
- [Project Structure](#-project-structure)
- [How It Works](#-how-it-works)
- [State Management](#-state-management)
- [Testing](#-testing)
- [Development Plan](#-development-plan)
- [Contributing](#-contributing)

---

## ‚ú® Features

| Feature | Description |
|---------|-------------|
| üîç **Deep Research** | Autonomous agent that searches the web via DuckDuckGo, intelligently selects relevant sources, scrapes content using Trafilatura, and summarizes findings |
| üß† **RAG-Powered Writing** | Uses SentenceTransformers + FAISS to retrieve only relevant research chunks (~600-1000 tokens) per section, avoiding rate limits and improving context quality |
| üéØ **Multi-Agent Evaluation** | Parallel critique system using 3 specialized AI models to evaluate drafts on SEO, Engagement, and Logic before final optimization |
| ü§ñ‚û°Ô∏èüë§ **Humanization Loop** | Reflexion-based loop achieving **0% AI detection** - detects "AI artifacts" (hedging, nominalizations, sensory vacuum) and rewrites until text is indistinguishable from human writing |
| ‚ùì **FAQ Generation** | Automatically generates 5-7 FAQ items from research data and article content, formatted for rich snippets |
| üìä **Keyword Analysis** | Extracts primary, secondary, and LSI keywords with density analysis and SEO recommendations |
| üîó **Structured Linking** | Generates 3-5 internal link suggestions and 2-4 authoritative external source citations with placement context |
| üìÅ **Virtual Filesystem (VFS)** | In-memory file system that decouples working memory from research data, enabling infinite research depth without context window overflow |
| üìù **Structured Planning** | AI Planner agent that breaks down topics into executable research and writing tasks with logical flow |
| ‚úçÔ∏è **Anti-AI Writing Constraints** | Writer uses detailed before/after examples to enforce human voice - bans 20+ robotic words and enforces sentence burstiness |
| üíæ **Persistence** | Jobs persisted to SQLite with async checkpointing, enabling resume capability for long-running tasks |
| üåê **Service Layer** | Production-ready FastAPI backend for managing content generation jobs via REST API |
| üîÑ **Conditional Routing** | LangGraph-powered state machine with intelligent task routing between research, writing, evaluation, and humanization phases |

---

## üèó Architecture

The system is built on **LangGraph** with a "Supervisor" pattern orchestrating five subgraphs:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              MAIN GRAPH                                     ‚îÇ
‚îÇ                     (Supervisor / Orchestrator)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                           ‚îÇ
‚îÇ  ‚îÇ   PLANNER    ‚îÇ ‚îÄ‚îÄ‚ñ∫ Creates structured task list (research + write tasks) ‚îÇ
‚îÇ  ‚îÇ  (GPT-120B)  ‚îÇ                                                           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                           ‚îÇ
‚îÇ          ‚îÇ                                                                  ‚îÇ
‚îÇ          ‚ñº                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                     CONDITIONAL ROUTER                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ         (Routes to Researcher/Writer based on task type)             ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ          ‚îÇ                                     ‚îÇ                            ‚îÇ
‚îÇ          ‚ñº                                     ‚ñº                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ  ‚îÇ  RESEARCHER  ‚îÇ                      ‚îÇ    WRITER    ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ  (GPT-20B)   ‚îÇ                      ‚îÇ  (GPT-120B)  ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ                      ‚îÇ              ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                      ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îÇ  Search  ‚îÇ ‚îÇ                      ‚îÇ ‚îÇ Gather   ‚îÇ ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îÇ  (DDG)   ‚îÇ ‚îÇ                      ‚îÇ ‚îÇ Context  ‚îÇ ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                      ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ      ‚ñº       ‚îÇ                      ‚îÇ      ‚ñº       ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                      ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îÇ  Select  ‚îÇ ‚îÇ                      ‚îÇ ‚îÇ  Write   ‚îÇ ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îÇ  URLs    ‚îÇ ‚îÇ                      ‚îÇ ‚îÇ Section  ‚îÇ ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                      ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ      ‚ñº       ‚îÇ                      ‚îÇ      ‚ñº       ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                      ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îÇ  Scrape  ‚îÇ ‚îÇ                      ‚îÇ ‚îÇ  Append  ‚îÇ ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Content  ‚îÇ ‚îÇ                      ‚îÇ ‚îÇ to Draft ‚îÇ ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                      ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ      ‚ñº       ‚îÇ                      ‚îÇ              ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                      ‚îÇ              ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îÇSummarize ‚îÇ ‚îÇ                      ‚îÇ              ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îÇ& Save VFS‚îÇ ‚îÇ                      ‚îÇ              ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                      ‚îÇ              ‚îÇ                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ          ‚îÇ                                     ‚îÇ                            ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ
‚îÇ                         ‚ñº                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                         EVALUATOR SUBGRAPH                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                    (Parallel Multi-Model Critique)                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ    QWEN     ‚îÇ   ‚îÇ    KIMI     ‚îÇ   ‚îÇ   LLAMA 4   ‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ  (32B)      ‚îÇ   ‚îÇ(k2-instruct)‚îÇ   ‚îÇ (Maverick)  ‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ             ‚îÇ   ‚îÇ             ‚îÇ   ‚îÇ             ‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ SEO &       ‚îÇ   ‚îÇ Engagement  ‚îÇ   ‚îÇ Logic &     ‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ Structure   ‚îÇ   ‚îÇ & Tone      ‚îÇ   ‚îÇ Accuracy    ‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ           ‚îÇ                 ‚îÇ                 ‚îÇ                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                             ‚ñº                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ    OPTIMIZER    ‚îÇ                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ   (GPT-120B)    ‚îÇ                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ                 ‚îÇ                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ Synthesizes all ‚îÇ                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ critiques and   ‚îÇ                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ rewrites draft  ‚îÇ                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                ‚îÇ                                            ‚îÇ
‚îÇ                                ‚ñº                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                      HUMANIZER SUBGRAPH                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                  (Reflexion Loop - up to 3 iterations)               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ  HUMANIZATION   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ    REFINER      ‚îÇ                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ     CRITIC      ‚îÇ         ‚îÇ                 ‚îÇ                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ                 ‚îÇ         ‚îÇ Chain of Density‚îÇ                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ Detects:        ‚îÇ         ‚îÇ Sensory Inject  ‚îÇ                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ - Hedging       ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ Kill Hedges     ‚îÇ                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ - Connectors    ‚îÇ  Loop   ‚îÇ Prune Connectors‚îÇ                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ - Nominalization‚îÇ  if     ‚îÇ                 ‚îÇ                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ - Sensory Vacuum‚îÇ score>3 ‚îÇ                 ‚îÇ                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                ‚îÇ                                            ‚îÇ
‚îÇ                                ‚ñº                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                    SEO ANALYSIS (PARALLEL)                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ     FAQ     ‚îÇ   ‚îÇ  KEYWORDS   ‚îÇ   ‚îÇ   LINKING   ‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ  GENERATOR  ‚îÇ   ‚îÇ  ANALYZER   ‚îÇ   ‚îÇ  SUGGESTER  ‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ             ‚îÇ   ‚îÇ             ‚îÇ   ‚îÇ             ‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ 5-7 Q&A     ‚îÇ   ‚îÇ Primary +   ‚îÇ   ‚îÇ 3-5 Internal‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ from        ‚îÇ   ‚îÇ Secondary + ‚îÇ   ‚îÇ 2-4 External‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ research    ‚îÇ   ‚îÇ LSI + Density‚îÇ  ‚îÇ with context‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                ‚îÇ                                            ‚îÇ
‚îÇ                                ‚ñº                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                         FINALIZER                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     (SEO metadata + FAQ section + Keyword Report + Link Strategy)    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                ‚îÇ                                            ‚îÇ
‚îÇ                                ‚ñº                                            ‚îÇ
‚îÇ                        [final_article.md]                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Models Used

| Purpose | Model | Provider | Temperature |
|---------|-------|----------|-------------|
| **Planning** | `openai/gpt-oss-120b` | Groq | 0.2 |
| **Research** | `openai/gpt-oss-20b` | Groq | 0.0 |
| **Writing** | `openai/gpt-oss-120b` | Groq | 0.4 |
| **Critique (SEO/Structure)** | `qwen/qwen3-32b` | Groq | 0.1 |
| **Critique (Engagement)** | `moonshotai/kimi-k2-instruct` | Groq | 0.2 |
| **Critique (Logic)** | `meta-llama/llama-4-maverick-17b-128e-instruct` | Groq | 0.1 |
| **Optimization** | `openai/gpt-oss-120b` | Groq | 0.2 |

---

## üõ† Tech Stack

| Category | Technologies |
|----------|--------------|
| **Framework** | LangGraph (State Machine Orchestration) |
| **LLM Provider** | Groq (Fast Inference) |
| **RAG / Embeddings** | SentenceTransformers + FAISS |
| **API** | FastAPI + Uvicorn |
| **Web Search** | DDGS (DuckDuckGo Search) |
| **Web Scraping** | Trafilatura |
| **Persistence** | SQLite (Async) |
| **Validation** | Pydantic |
| **Testing** | Pytest |
| **Environment** | Python-dotenv |

---

## üì¶ Installation

### Prerequisites

- Python 3.10+
- Groq API Key ([Get one here](https://console.groq.com))

### Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/raheelism/article-agent.git
   cd article-agent
   ```

2. **Create virtual environment** (recommended)
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # Linux/macOS
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   # Create .env file
   echo "GROQ_API_KEY=your_groq_api_key_here" > .env
   ```

---

## ‚öô Configuration

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `GROQ_API_KEY` | ‚úÖ Yes | Your Groq API key for LLM access |

### Model Configuration

Models are configured in `app/core/llm.py`. Each model factory function can be customized:

```python
# Example: Modify temperature for writer
def get_writer_model():
    return get_model("openai/gpt-oss-120b", temperature=0.4)
```

---

## üöÄ Usage

### 1. CLI Mode (Development/Testing)

Run the agent directly from command line:

```bash
# Windows (PowerShell) or Linux/macOS
python scripts/run_agent.py "Benefits of Remote Work" --word-count 1000
```

**CLI Arguments:**

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `topic` | string | required | The article topic to write about |
| `--word-count` | int | 1500 | Target word count for the article |

**Output:** Final article saved to `Generated articles/{Topic}_{Timestamp}.md`

**Example Output Path:** `Generated articles/Benefits_of_Remote_Work_20251208_143052.md`

### Output Structure

The generated article includes:

```markdown
---
title: "SEO-Optimized Title (max 60 chars)"
meta_description: "Compelling description (max 160 chars)"
primary_keyword: "main keyword"
---

# Article Title

[Article content with proper H1/H2/H3 hierarchy...]

---

## ‚ùì Frequently Asked Questions

### What is [topic]?
[Answer based on research...]

### How do I [action]?
[Practical answer...]

[5-7 FAQ items total]

---

## üìä SEO Analysis Report

### Keywords

**Primary Keyword:** `main keyword phrase`

**Secondary Keywords:**
- keyword 1
- keyword 2

**LSI Keywords:**
- related term 1
- related term 2

**Keyword Density:**
| Keyword | Density |
|---------|---------|
| main keyword | 1.5% |

**SEO Recommendations:**
- ‚úÖ Primary keyword density is optimal (1.5%)
- üí° Consider adding primary keyword to H2 headings

### üîó Linking Strategy

**Internal Links (3-5):**
| Anchor Text | Target Page | Context |
|-------------|-------------|---------|
| productivity tools | Best Tools Guide | In tools section |

**External Links (Authoritative Sources):**
| Source | Anchor Text | Placement |
|--------|-------------|-----------|
| Harvard Business Review | according to HBR | Support statistics |
```

### 2. API Mode (Production)

Start the FastAPI server:

```bash
# Windows (PowerShell) or Linux/macOS
uvicorn app.api.server:app --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

---

## üì° API Reference

### Base URL
```
http://localhost:8000
```

### Endpoints

#### Create Job
```http
POST /jobs
Content-Type: application/json

{
  "topic": "Future of AI in Healthcare",
  "word_count": 1500,
  "language": "English"
}
```

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "pending",
  "result": null
}
```

#### Get Job Status
```http
GET /jobs/{job_id}
```

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "completed",
  "result": "# Article Title\n\nArticle content..."
}
```

**Job Status Values:**
- `pending` - Job queued
- `running` - Job in progress
- `completed` - Job finished successfully
- `failed` - Job encountered an error

---

## üìÅ Project Structure

```
article-agent/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ planner.py          # Planner agent - creates task list
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ server.py           # FastAPI server with job management
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm.py              # LLM factory functions (Groq integration)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ state.py            # AgentState TypedDict definition
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vfs.py              # Virtual File System implementation
‚îÇ   ‚îú‚îÄ‚îÄ graphs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py        # Evaluator subgraph (parallel critiques)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ faq.py              # FAQ generator (from research data)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ humanizer.py        # Humanizer subgraph (reflexion loop)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ keyword_analyzer.py # Keyword extraction and density analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ linking.py          # Internal/external link suggestions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ researcher.py       # Researcher subgraph (search/scrape/summarize)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ writer.py           # Writer subgraph (RAG + draft generation)
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraper.py          # Web scraping with Trafilatura
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ search.py           # DDGS search provider
‚îÇ   ‚îî‚îÄ‚îÄ main_graph.py           # Main orchestration graph
‚îú‚îÄ‚îÄ plan/
‚îÇ   ‚îú‚îÄ‚îÄ phase_1_foundation/     # Phase 1 implementation docs
‚îÇ   ‚îú‚îÄ‚îÄ phase_2_agent_graph/    # Phase 2 implementation docs
‚îÇ   ‚îú‚îÄ‚îÄ phase_3_service_quality/# Phase 3 implementation docs
‚îÇ   ‚îú‚îÄ‚îÄ phase_4_humanization/   # Phase 4 implementation docs
‚îÇ   ‚îú‚îÄ‚îÄ system_architecture/    # System architecture docs
‚îÇ   ‚îî‚îÄ‚îÄ README.md               # Development plan overview
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ run_agent.py            # CLI entry point
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_evaluator.py       # Evaluator tests
‚îÇ   ‚îú‚îÄ‚îÄ test_humanizer.py       # Humanizer tests
‚îÇ   ‚îú‚îÄ‚îÄ test_tools.py           # Tools tests
‚îÇ   ‚îî‚îÄ‚îÄ test_vfs.py             # VFS tests
‚îú‚îÄ‚îÄ Generated articles/         # Output folder for generated articles
‚îÇ   ‚îî‚îÄ‚îÄ {Topic}_{Timestamp}.md  # Articles named by topic and timestamp
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ requirements.md             # Project requirements spec
‚îî‚îÄ‚îÄ README.md                   # This file
```

---

## üîÑ How It Works

### 1. Planning Phase
The Planner agent receives the topic and generates a structured task list:

```json
[
  {"id": 1, "type": "research", "description": "Research main topic keywords..."},
  {"id": 2, "type": "research", "description": "Find competitor articles..."},
  {"id": 3, "type": "write", "description": "Write introduction..."},
  {"id": 4, "type": "write", "description": "Write main body sections..."},
  {"id": 5, "type": "write", "description": "Write conclusion..."}
]
```

### 2. Research Phase
For each `research` task:
1. **Search** - Query DuckDuckGo for relevant results
2. **Select** - LLM picks top 2 most relevant URLs
3. **Scrape** - Extract content using Trafilatura
4. **Summarize** - LLM extracts key facts and saves to VFS

### 3. Writing Phase (RAG-Powered)
For each `write` task:
1. **Chunk Research** - Split research summaries into paragraphs
2. **Embed & Index** - Use SentenceTransformers to embed chunks, build FAISS index
3. **Retrieve Context** - Find top 4 most relevant chunks (~600-1000 tokens) for the current section
4. **Generate Content** - LLM writes section with "Absolute Mode" constraints (no robotic words, enforced burstiness)
5. **Append** - Add new content to `draft.md` in VFS

### 4. Evaluation Phase
Three parallel critics analyze the draft:

| Critic | Focus Areas |
|--------|-------------|
| **Qwen** | Header structure, keyword usage, internal linking, formatting |
| **Kimi** | Tone, flow, storytelling, hooks, conclusions |
| **Llama 4** | Logical consistency, clarity, factual accuracy, depth |

### 5. Optimization Phase
The Optimizer (GPT-120B):
- Synthesizes all three critiques
- Rewrites the article addressing all issues
- Maintains Markdown formatting

### 6. Humanization Phase (Reflexion Loop)
Up to 3 iterations of:
1. **Humanization Critic** - Analyzes for AI artifacts with detailed pattern matching:
   - Hedging ("It is important to note that..." ‚Üí direct statements)
   - Connector overuse ("Moreover, Furthermore, Additionally" ‚Üí natural flow)
   - Nominalization ("made a decision" ‚Üí "decided")
   - Sensory vacuum (abstract ‚Üí physical details)
   - AI vocabulary detection (delve, tapestry, leverage, robust, etc.)
   - Burstiness score (sentence length variety)
2. **Refiner Agent** - Rewrites using example-driven transformations:
   - Before/after patterns for hedge elimination
   - Sensory injection templates
   - Connector pruning with rhythm patterns
   - AI vocabulary replacement guide (20+ words)
3. **Loop Check** - If AI Artifact Score > 3, repeat; otherwise exit

**Banned Words List:**
```
delve, tapestry, landscape, unleash, foster, paramount, underscores,
game-changer, multifaceted, holistic, leverage, synergy, robust,
streamline, cutting-edge, revolutionary, transformative, comprehensive,
facilitate, utilize
```

### 7. SEO Analysis (Parallel)
After humanization, three agents run in parallel:

1. **FAQ Generator** - Creates 5-7 Q&A pairs from research data
2. **Keyword Analyzer** - Extracts primary/secondary/LSI keywords with density analysis
3. **Linking Suggester** - Generates 3-5 internal + 2-4 external link suggestions

### 8. Finalization
- Generates SEO metadata (title tag, meta description)
- Appends FAQ section to article
- Appends keyword analysis report
- Appends linking strategy tables
- Saves final article to `Generated articles/{Topic}_{Timestamp}.md`

---

## üìä State Management

### AgentState Schema

```python
class AgentState(TypedDict):
    # User Inputs
    topic: str              # Article topic
    word_count: int         # Target word count
    language: str           # Output language (default: English)
    
    # Execution State
    plan: List[Task]        # List of tasks to execute
    current_task_index: int # Current task pointer
    
    # Virtual File System (serializable)
    vfs_data: Dict[str, Dict]  # Filename -> File content/metadata
    
    # SEO Analysis Results
    faqs: List[FAQItem]           # Generated FAQ section
    keyword_report: KeywordReport # Keyword analysis
    linking_report: LinkingReport # Link suggestions
    
    # Logging
    logs: List[str]         # Execution logs (append-only)
```

### SEO Report Schemas

```python
class FAQItem(TypedDict):
    question: str
    answer: str

class KeywordReport(TypedDict):
    primary_keyword: str
    secondary_keywords: List[str]
    lsi_keywords: List[str]
    keyword_density: Dict[str, float]
    recommendations: List[str]
    total_words: int

class LinkingReport(TypedDict):
    internal_links: List[InternalLink]  # anchor, target, context
    external_links: List[ExternalLink]  # source, url, anchor, placement
```

### Task Schema

```python
class Task(TypedDict):
    id: int                 # Task ID
    description: str        # Task description
    type: str              # 'research' | 'write' | 'review'
    status: str            # 'pending' | 'completed' | 'failed'
    params: Dict           # Additional parameters
```

### Virtual File System

The VFS provides an abstraction layer for storing intermediate artifacts:

```python
vfs = VFS()
vfs.write_file("research/topic1.md", "Summary content...", metadata={"url": "https://..."})
vfs.write_file("draft.md", "Article content...")
content = vfs.read_file("draft.md")
files = vfs.list_files()  # ["research/topic1.md", "draft.md"]
```

---

## üß™ Testing

Run the test suite:

```bash
# Windows (PowerShell) or Linux/macOS
pytest

# With verbose output
pytest -v

# Run specific test file
pytest tests/test_vfs.py
pytest tests/test_humanizer.py
```

---

## üìà Development Plan

The project follows a phased development approach:

| Phase | Focus | Status |
|-------|-------|--------|
| **Phase 1** | Foundation (Core infrastructure, VFS, LLM) | ‚úÖ Complete |
| **Phase 2** | Agent Graph (Subgraphs, routing, orchestration) | ‚úÖ Complete |
| **Phase 3** | Service & Quality (API, persistence, evaluation) | ‚úÖ Complete |
| **Phase 4** | Humanization (RAG writing, reflexion loop, anti-AI prompts) | ‚úÖ Complete |
| **Phase 5** | SEO Analysis (FAQ generator, keyword analyzer, linking suggester) | ‚úÖ Complete |

See [`plan/`](plan/) directory for detailed implementation guides.

---

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## üìÑ License

This project is open source and available under the [MIT License](LICENSE).

---

<p align="center">
  Made with ‚ù§Ô∏è by <a href="https://github.com/raheelism">raheelism</a>
</p>
