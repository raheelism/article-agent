# Article Agent

<p align="center">
  <strong>ğŸ¤– AI-Powered Article Generation System</strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/AI%20Detection-0%25-brightgreen?style=for-the-badge" alt="0% AI Detection" />
  <img src="https://img.shields.io/badge/Human--Like%20Output-100%25-blue?style=for-the-badge" alt="100% Human-Like" />
</p>

A sophisticated "Deep Agent" system for generating high-quality, SEO-optimized articles using **LangGraph**, **FastAPI**, and **Groq** (Llama 4 / GPT OSS / Qwen / Kimi). The system employs a multi-agent architecture with autonomous research, structured writing, and parallel evaluation capabilities.

> **ğŸ¯ Key Achievement:** Generated articles pass AI detection tools with **0% AI-detected content**, thanks to the advanced Humanization Loop with reflexion-based rewriting and anti-AI writing constraints.

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [API Reference](#-api-reference)
- [Project Structure](#-project-structure)
- [How It Works](#-how-it-works)
- [State Management](#-state-management)
- [Example Input/Output](#-example-inputoutput)
- [Design Decisions](#-design-decisions)
- [Testing](#-testing)
- [Development Plan](#-development-plan)
- [Contributing](#-contributing)

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ” **Deep Research** | Autonomous agent that searches the web via DuckDuckGo, intelligently selects relevant sources, scrapes content using Trafilatura, and summarizes findings |
| ğŸ§  **RAG-Powered Writing** | Uses SentenceTransformers + FAISS to retrieve only relevant research chunks (~600-1000 tokens) per section, avoiding rate limits and improving context quality |
| ğŸ¯ **Multi-Agent Evaluation** | Parallel critique system using 3 specialized AI models to evaluate drafts on SEO, Engagement, and Logic before final optimization |
| ğŸ¤–â¡ï¸ğŸ‘¤ **Humanization Loop** | Reflexion-based loop achieving **0% AI detection** - detects "AI artifacts" (hedging, nominalizations, sensory vacuum) and rewrites until text is indistinguishable from human writing |
| â“ **FAQ Generation** | Automatically generates 5-7 FAQ items from research data and article content, formatted for rich snippets |
| ğŸ“Š **Keyword Analysis** | Extracts primary, secondary, and LSI keywords with density analysis and SEO recommendations |
| ğŸ”— **Structured Linking** | Generates 3-5 internal link suggestions and 2-4 authoritative external source citations with placement context |
| ğŸ“ **Virtual Filesystem (VFS)** | In-memory file system that decouples working memory from research data, enabling infinite research depth without context window overflow |
| ğŸ“ **Structured Planning** | AI Planner agent that breaks down topics into executable research and writing tasks with logical flow |
| âœï¸ **Anti-AI Writing Constraints** | Writer uses detailed before/after examples to enforce human voice - bans 20+ robotic words and enforces sentence burstiness |
| ğŸ’¾ **Persistence** | Jobs persisted to SQLite with async checkpointing, enabling resume capability for long-running tasks |
| ğŸŒ **Service Layer** | Production-ready FastAPI backend for managing content generation jobs via REST API |
| ğŸ”„ **Conditional Routing** | LangGraph-powered state machine with intelligent task routing between research, writing, evaluation, and humanization phases |

---

## ğŸ— Architecture

The system is built on **LangGraph** with a "Supervisor" pattern orchestrating five subgraphs:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              MAIN GRAPH                                     â”‚
â”‚                     (Supervisor / Orchestrator)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                           â”‚
â”‚  â”‚   PLANNER    â”‚ â”€â”€â–º Creates structured task list (research + write tasks) â”‚
â”‚  â”‚  (GPT-120B)  â”‚                                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                           â”‚
â”‚          â”‚                                                                  â”‚
â”‚          â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                     CONDITIONAL ROUTER                               â”‚   â”‚
â”‚  â”‚         (Routes to Researcher/Writer based on task type)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â”‚                                     â”‚                            â”‚
â”‚          â–¼                                     â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  RESEARCHER  â”‚                      â”‚    WRITER    â”‚                     â”‚
â”‚  â”‚  (GPT-20B)   â”‚                      â”‚  (GPT-120B)  â”‚                     â”‚
â”‚  â”‚              â”‚                      â”‚              â”‚                     â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                     â”‚
â”‚  â”‚ â”‚  Search  â”‚ â”‚                      â”‚ â”‚ Gather   â”‚ â”‚                     â”‚
â”‚  â”‚ â”‚  (DDG)   â”‚ â”‚                      â”‚ â”‚ Context  â”‚ â”‚                     â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚                      â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚                     â”‚
â”‚  â”‚      â–¼       â”‚                      â”‚      â–¼       â”‚                     â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                     â”‚
â”‚  â”‚ â”‚  Select  â”‚ â”‚                      â”‚ â”‚  Write   â”‚ â”‚                     â”‚
â”‚  â”‚ â”‚  URLs    â”‚ â”‚                      â”‚ â”‚ Section  â”‚ â”‚                     â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚                      â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚                     â”‚
â”‚  â”‚      â–¼       â”‚                      â”‚      â–¼       â”‚                     â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                     â”‚
â”‚  â”‚ â”‚  Scrape  â”‚ â”‚                      â”‚ â”‚  Append  â”‚ â”‚                     â”‚
â”‚  â”‚ â”‚ Content  â”‚ â”‚                      â”‚ â”‚ to Draft â”‚ â”‚                     â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚                      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                     â”‚
â”‚  â”‚      â–¼       â”‚                      â”‚              â”‚                     â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                      â”‚              â”‚                     â”‚
â”‚  â”‚ â”‚Summarize â”‚ â”‚                      â”‚              â”‚                     â”‚
â”‚  â”‚ â”‚& Save VFSâ”‚ â”‚                      â”‚              â”‚                     â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                      â”‚              â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚          â”‚                                     â”‚                            â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         EVALUATOR SUBGRAPH                           â”‚   â”‚
â”‚  â”‚                    (Parallel Multi-Model Critique)                   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚   â”‚
â”‚  â”‚    â”‚    QWEN     â”‚   â”‚    KIMI     â”‚   â”‚   LLAMA 4   â”‚               â”‚   â”‚
â”‚  â”‚    â”‚  (32B)      â”‚   â”‚(k2-instruct)â”‚   â”‚ (Maverick)  â”‚               â”‚   â”‚
â”‚  â”‚    â”‚             â”‚   â”‚             â”‚   â”‚             â”‚               â”‚   â”‚
â”‚  â”‚    â”‚ SEO &       â”‚   â”‚ Engagement  â”‚   â”‚ Logic &     â”‚               â”‚   â”‚
â”‚  â”‚    â”‚ Structure   â”‚   â”‚ & Tone      â”‚   â”‚ Accuracy    â”‚               â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜               â”‚   â”‚
â”‚  â”‚           â”‚                 â”‚                 â”‚                      â”‚   â”‚
â”‚  â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚   â”‚
â”‚  â”‚                             â–¼                                        â”‚   â”‚
â”‚  â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚   â”‚
â”‚  â”‚                    â”‚    OPTIMIZER    â”‚                               â”‚   â”‚
â”‚  â”‚                    â”‚   (GPT-120B)    â”‚                               â”‚   â”‚
â”‚  â”‚                    â”‚                 â”‚                               â”‚   â”‚
â”‚  â”‚                    â”‚ Synthesizes all â”‚                               â”‚   â”‚
â”‚  â”‚                    â”‚ critiques and   â”‚                               â”‚   â”‚
â”‚  â”‚                    â”‚ rewrites draft  â”‚                               â”‚   â”‚
â”‚  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      HUMANIZER SUBGRAPH                              â”‚   â”‚
â”‚  â”‚                  (Reflexion Loop - up to 3 iterations)               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚   â”‚
â”‚  â”‚    â”‚  HUMANIZATION   â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚    REFINER      â”‚                   â”‚   â”‚
â”‚  â”‚    â”‚     CRITIC      â”‚         â”‚                 â”‚                   â”‚   â”‚
â”‚  â”‚    â”‚                 â”‚         â”‚ Chain of Densityâ”‚                   â”‚   â”‚
â”‚  â”‚    â”‚ Detects:        â”‚         â”‚ Sensory Inject  â”‚                   â”‚   â”‚
â”‚  â”‚    â”‚ - Hedging       â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚ Kill Hedges     â”‚                   â”‚   â”‚
â”‚  â”‚    â”‚ - Connectors    â”‚  Loop   â”‚ Prune Connectorsâ”‚                   â”‚   â”‚
â”‚  â”‚    â”‚ - Nominalizationâ”‚  if     â”‚                 â”‚                   â”‚   â”‚
â”‚  â”‚    â”‚ - Sensory Vacuumâ”‚ score>3 â”‚                 â”‚                   â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    SEO ANALYSIS (PARALLEL)                           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚   â”‚
â”‚  â”‚    â”‚     FAQ     â”‚   â”‚  KEYWORDS   â”‚   â”‚   LINKING   â”‚               â”‚   â”‚
â”‚  â”‚    â”‚  GENERATOR  â”‚   â”‚  ANALYZER   â”‚   â”‚  SUGGESTER  â”‚               â”‚   â”‚
â”‚  â”‚    â”‚             â”‚   â”‚             â”‚   â”‚             â”‚               â”‚   â”‚
â”‚  â”‚    â”‚ 5-7 Q&A     â”‚   â”‚ Primary +   â”‚   â”‚ 3-5 Internalâ”‚               â”‚   â”‚
â”‚  â”‚    â”‚ from        â”‚   â”‚ Secondary + â”‚   â”‚ 2-4 Externalâ”‚               â”‚   â”‚
â”‚  â”‚    â”‚ research    â”‚   â”‚ LSI + Densityâ”‚  â”‚ with contextâ”‚               â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         FINALIZER                                    â”‚   â”‚
â”‚  â”‚     (SEO metadata + FAQ section + Keyword Report + Link Strategy)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚                        [final_article.md]                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Models Used

| Purpose | Model | Provider | Temperature |
|---------|-------|----------|-------------|
| **Planning** | `openai/gpt-oss-120b` | Groq | 0.2 |
| **Research** | `openai/gpt-oss-20b` | Groq | 0.0 |
| **Writing** | `openai/gpt-oss-120b` | Groq | 0.4 |
| **Critique (SEO/Structure)** | `qwen/qwen3-32b` | Groq | 0.1 |
| **Critique (Engagement)** | `moonshotai/kimi-k2-instruct` | Groq | 0.2 |
| **Critique (Logic)** | `meta-llama/llama-4-maverick-17b-128e-instruct` | Groq | 0.1 |
| **Optimization** | `openai/gpt-oss-120b` | Groq | 0.2 |

---

## ğŸ›  Tech Stack

| Category | Technologies |
|----------|--------------|
| **Framework** | LangGraph (State Machine Orchestration) |
| **LLM Provider** | Groq (Fast Inference) |
| **RAG / Embeddings** | SentenceTransformers + FAISS |
| **API** | FastAPI + Uvicorn |
| **Web Search** | DDGS (DuckDuckGo Search) |
| **Web Scraping** | Trafilatura |
| **Persistence** | SQLite (Async) |
| **Validation** | Pydantic |
| **Testing** | Pytest |
| **Environment** | Python-dotenv |

---

## ğŸ“¦ Installation

### Prerequisites

- Python 3.10+
- Groq API Key ([Get one here](https://console.groq.com))

### Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/raheelism/article-agent.git
   cd article-agent
   ```

2. **Create virtual environment** (recommended)
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # Linux/macOS
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   # Create .env file
   echo "GROQ_API_KEY=your_groq_api_key_here" > .env
   ```

---

## âš™ Configuration

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `GROQ_API_KEY` | âœ… Yes | Your Groq API key for LLM access |

### Model Configuration

Models are configured in `app/core/llm.py`. Each model factory function can be customized:

```python
# Example: Modify temperature for writer
def get_writer_model():
    return get_model("openai/gpt-oss-120b", temperature=0.4)
```

---

## ğŸš€ Usage

### 1. CLI Mode (Development/Testing)

Run the agent directly from command line:

```bash
# Windows (PowerShell) or Linux/macOS
python scripts/run_agent.py "Benefits of Remote Work" --word-count 1000
```

**CLI Arguments:**

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `topic` | string | required | The article topic to write about |
| `--word-count` | int | 1500 | Target word count for the article |

**Output:** Final article saved to `Generated articles/{Topic}_{Timestamp}.md`

**Example Output Path:** `Generated articles/Benefits_of_Remote_Work_20251208_143052.md`

### Output Structure

The generated article includes:

```markdown
---
title: "SEO-Optimized Title (max 60 chars)"
meta_description: "Compelling description (max 160 chars)"
primary_keyword: "main keyword"
---

# Article Title

[Article content with proper H1/H2/H3 hierarchy...]

---

## â“ Frequently Asked Questions

### What is [topic]?
[Answer based on research...]

### How do I [action]?
[Practical answer...]

[5-7 FAQ items total]

---

## ğŸ“Š SEO Analysis Report

### Keywords

**Primary Keyword:** `main keyword phrase`

**Secondary Keywords:**
- keyword 1
- keyword 2

**LSI Keywords:**
- related term 1
- related term 2

**Keyword Density:**
| Keyword | Density |
|---------|---------|
| main keyword | 1.5% |

**SEO Recommendations:**
- âœ… Primary keyword density is optimal (1.5%)
- ğŸ’¡ Consider adding primary keyword to H2 headings

### ğŸ”— Linking Strategy

**Internal Links (3-5):**
| Anchor Text | Target Page | Context |
|-------------|-------------|---------|
| productivity tools | Best Tools Guide | In tools section |

**External Links (Authoritative Sources):**
| Source | Anchor Text | Placement |
|--------|-------------|-----------|
| Harvard Business Review | according to HBR | Support statistics |
```

### 2. API Mode (Production)

Start the FastAPI server:

```bash
# Windows (PowerShell) or Linux/macOS
uvicorn app.api.server:app --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

---

## ğŸ“¡ API Reference

### Base URL
```
http://localhost:8000
```

### Endpoints

#### Create Job
```http
POST /jobs
Content-Type: application/json

{
  "topic": "Future of AI in Healthcare",
  "word_count": 1500,
  "language": "English"
}
```

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "pending",
  "result": null
}
```

#### Get Job Status
```http
GET /jobs/{job_id}
```

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "completed",
  "result": "# Article Title\n\nArticle content..."
}
```

**Job Status Values:**
- `pending` - Job queued
- `running` - Job in progress
- `completed` - Job finished successfully
- `failed` - Job encountered an error

---

## ğŸ“ Project Structure

```
article-agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â””â”€â”€ planner.py          # Planner agent - creates task list
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ server.py           # FastAPI server with job management
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ llm.py              # LLM factory functions (Groq integration)
â”‚   â”‚   â”œâ”€â”€ state.py            # AgentState TypedDict definition
â”‚   â”‚   â””â”€â”€ vfs.py              # Virtual File System implementation
â”‚   â”œâ”€â”€ graphs/
â”‚   â”‚   â”œâ”€â”€ evaluator.py        # Evaluator subgraph (parallel critiques)
â”‚   â”‚   â”œâ”€â”€ faq.py              # FAQ generator (from research data)
â”‚   â”‚   â”œâ”€â”€ humanizer.py        # Humanizer subgraph (reflexion loop)
â”‚   â”‚   â”œâ”€â”€ keyword_analyzer.py # Keyword extraction and density analysis
â”‚   â”‚   â”œâ”€â”€ linking.py          # Internal/external link suggestions
â”‚   â”‚   â”œâ”€â”€ researcher.py       # Researcher subgraph (search/scrape/summarize)
â”‚   â”‚   â””â”€â”€ writer.py           # Writer subgraph (RAG + draft generation)
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ scraper.py          # Web scraping with Trafilatura
â”‚   â”‚   â””â”€â”€ search.py           # DDGS search provider
â”‚   â””â”€â”€ main_graph.py           # Main orchestration graph
â”œâ”€â”€ plan/
â”‚   â”œâ”€â”€ phase_1_foundation/     # Phase 1 implementation docs
â”‚   â”œâ”€â”€ phase_2_agent_graph/    # Phase 2 implementation docs
â”‚   â”œâ”€â”€ phase_3_service_quality/# Phase 3 implementation docs
â”‚   â”œâ”€â”€ phase_4_humanization/   # Phase 4 implementation docs
â”‚   â”œâ”€â”€ system_architecture/    # System architecture docs
â”‚   â””â”€â”€ README.md               # Development plan overview
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_agent.py            # CLI entry point
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_evaluator.py       # Evaluator tests
â”‚   â”œâ”€â”€ test_humanizer.py       # Humanizer tests
â”‚   â”œâ”€â”€ test_tools.py           # Tools tests
â”‚   â””â”€â”€ test_vfs.py             # VFS tests
â”œâ”€â”€ generated articles/         # Output folder for generated articles
â”‚   â””â”€â”€ {Topic}_{Timestamp}.md  # Articles named by topic and timestamp
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ requirements.md             # Project requirements spec
â””â”€â”€ README.md                   # This file
```

---

## ğŸ”„ How It Works

### 1. Planning Phase
The Planner agent receives the topic and generates a structured task list:

```json
[
  {"id": 1, "type": "research", "description": "Research main topic keywords..."},
  {"id": 2, "type": "research", "description": "Find competitor articles..."},
  {"id": 3, "type": "write", "description": "Write introduction..."},
  {"id": 4, "type": "write", "description": "Write main body sections..."},
  {"id": 5, "type": "write", "description": "Write conclusion..."}
]
```

### 2. Research Phase
For each `research` task:
1. **Search** - Query DuckDuckGo for relevant results
2. **Select** - LLM picks top 2 most relevant URLs
3. **Scrape** - Extract content using Trafilatura
4. **Summarize** - LLM extracts key facts and saves to VFS

### 3. Writing Phase (RAG-Powered)
For each `write` task:
1. **Chunk Research** - Split research summaries into paragraphs
2. **Embed & Index** - Use SentenceTransformers to embed chunks, build FAISS index
3. **Retrieve Context** - Find top 4 most relevant chunks (~600-1000 tokens) for the current section
4. **Generate Content** - LLM writes section with "Absolute Mode" constraints (no robotic words, enforced burstiness)
5. **Append** - Add new content to `draft.md` in VFS

### 4. Evaluation Phase
Three parallel critics analyze the draft:

| Critic | Focus Areas |
|--------|-------------|
| **Qwen** | Header structure, keyword usage, internal linking, formatting |
| **Kimi** | Tone, flow, storytelling, hooks, conclusions |
| **Llama 4** | Logical consistency, clarity, factual accuracy, depth |

### 5. Optimization Phase
The Optimizer (GPT-120B):
- Synthesizes all three critiques
- Rewrites the article addressing all issues
- Maintains Markdown formatting

### 6. Humanization Phase (Reflexion Loop)
Up to 3 iterations of:
1. **Humanization Critic** - Analyzes for AI artifacts with detailed pattern matching:
   - Hedging ("It is important to note that..." â†’ direct statements)
   - Connector overuse ("Moreover, Furthermore, Additionally" â†’ natural flow)
   - Nominalization ("made a decision" â†’ "decided")
   - Sensory vacuum (abstract â†’ physical details)
   - AI vocabulary detection (delve, tapestry, leverage, robust, etc.)
   - Burstiness score (sentence length variety)
2. **Refiner Agent** - Rewrites using example-driven transformations:
   - Before/after patterns for hedge elimination
   - Sensory injection templates
   - Connector pruning with rhythm patterns
   - AI vocabulary replacement guide (20+ words)
3. **Loop Check** - If AI Artifact Score > 3, repeat; otherwise exit

**Banned Words List:**
```
delve, tapestry, landscape, unleash, foster, paramount, underscores,
game-changer, multifaceted, holistic, leverage, synergy, robust,
streamline, cutting-edge, revolutionary, transformative, comprehensive,
facilitate, utilize
```

### 7. SEO Analysis (Parallel)
After humanization, three agents run in parallel:

1. **FAQ Generator** - Creates 5-7 Q&A pairs from research data
2. **Keyword Analyzer** - Extracts primary/secondary/LSI keywords with density analysis
3. **Linking Suggester** - Generates 3-5 internal + 2-4 external link suggestions

### 8. Finalization
- Generates SEO metadata (title tag, meta description)
- Appends FAQ section to article
- Appends keyword analysis report
- Appends linking strategy tables
- Saves final article to `Generated articles/{Topic}_{Timestamp}.md`

---

## ğŸ“Š State Management

### AgentState Schema

```python
class AgentState(TypedDict):
    # User Inputs
    topic: str              # Article topic
    word_count: int         # Target word count
    language: str           # Output language (default: English)
    
    # Execution State
    plan: List[Task]        # List of tasks to execute
    current_task_index: int # Current task pointer
    
    # Virtual File System (serializable)
    vfs_data: Dict[str, Dict]  # Filename -> File content/metadata
    
    # SEO Analysis Results
    faqs: List[FAQItem]           # Generated FAQ section
    keyword_report: KeywordReport # Keyword analysis
    linking_report: LinkingReport # Link suggestions
    
    # Logging
    logs: List[str]         # Execution logs (append-only)
```

### SEO Report Schemas

```python
class FAQItem(TypedDict):
    question: str
    answer: str

class KeywordReport(TypedDict):
    primary_keyword: str
    secondary_keywords: List[str]
    lsi_keywords: List[str]
    keyword_density: Dict[str, float]
    recommendations: List[str]
    total_words: int

class LinkingReport(TypedDict):
    internal_links: List[InternalLink]  # anchor, target, context
    external_links: List[ExternalLink]  # source, url, anchor, placement
```

### Task Schema

```python
class Task(TypedDict):
    id: int                 # Task ID
    description: str        # Task description
    type: str              # 'research' | 'write' | 'review'
    status: str            # 'pending' | 'completed' | 'failed'
    params: Dict           # Additional parameters
```

### Virtual File System

The VFS provides an abstraction layer for storing intermediate artifacts:

```python
vfs = VFS()
vfs.write_file("research/topic1.md", "Summary content...", metadata={"url": "https://..."})
vfs.write_file("draft.md", "Article content...")
content = vfs.read_file("draft.md")
files = vfs.list_files()  # ["research/topic1.md", "draft.md"]
```

---

## ğŸ§ª Testing

Run the test suite:

```bash
# Windows (PowerShell) or Linux/macOS
pytest

# With verbose output
pytest -v

# Run specific test file
pytest tests/test_vfs.py
pytest tests/test_humanizer.py
pytest tests/test_seo_agents.py
```

### Test Coverage

| Test File | What It Tests |
|-----------|---------------|
| `test_vfs.py` | Virtual File System operations (read, write, list, metadata) |
| `test_tools.py` | Search providers and web scrapers (mock implementations) |
| `test_evaluator.py` | Multi-model evaluation subgraph |
| `test_humanizer.py` | Humanization reflexion loop |
| `test_seo_agents.py` | FAQ, Keyword Analyzer, and Linking Suggester agents |

### Example Test Output

```bash
$ pytest -v
============================= test session starts =============================
collected 15 items

tests/test_vfs.py::test_vfs_basic_operations PASSED                     [  6%]
tests/test_vfs.py::test_vfs_overwrite PASSED                            [ 13%]
tests/test_tools.py::test_mock_search PASSED                            [ 20%]
tests/test_tools.py::test_mock_scraper PASSED                           [ 26%]
tests/test_seo_agents.py::TestFAQAgent::test_faq_state_structure PASSED [ 33%]
tests/test_seo_agents.py::TestKeywordAnalyzer::test_keyword_report PASSED [40%]
tests/test_seo_agents.py::TestLinkingSuggester::test_linking_report PASSED[46%]
...
============================= 15 passed in 2.31s ==============================
```

---

## ğŸ“ Example: Input â†’ Output

### Input

```bash
python scripts/run_agent.py "Fear of AI in Job Market" --word-count 1000
```

### Execution Log

```
Starting Agent for topic: Fear of AI in Job Market
--- PLANNING: Fear of AI in Job Market ---
  [Researcher] Searching for: Conduct keyword research for "fear of AI in job market"...
  [Researcher] Selected: ['https://hai.stanford.edu/...', 'https://learn.g2.com/...']
  [Researcher] Saved research/summary_3265888463256558867.md
  [Writer] Loading local embedding model from: models/all-MiniLM-L6-v2
  [Writer] Retrieved 4 chunks for context.
  [Writer] Writing section: Write the Introduction (â‰ˆ150 words)...
  [Writer] Writing section: Write Body Point 1 (â‰ˆ150 words)...
  ...
--- Evaluating Article ---
  [Optimizer] Optimizing draft based on critiques...
--- Humanizing Article ---
  [Humanizer] AI Artifact Score: 8
  [Humanizer] Check: Score=8, Iteration=1
--- Running SEO Analysis (FAQ + Keywords + Linking) ---
  [Keywords] Analysis complete. Total words: 1470
  [Linking] Generated 5 internal + 4 external links
  [FAQ] Formatted 3 FAQ items
--- Finalizing Article ---
  [Finalize] Article complete with FAQ (3 items), keywords, and linking strategy

--- Execution Complete ---
```

### Output (Excerpt)

**File:** `Generated articles/Fear_of_AI_in_Job_Market_20251208_183751.md`

```markdown
# AI and the Future of Work: Navigating Anxiety, Automation, and Opportunity

## Introduction

The elevator dinged at the 17th floor. The AI assistant had already rewrote 
Jordan's sales deck, slashed his bonus forecast, and signed him up for a 
reskilling webinar he didn't remember agreeing to. He stared at the glowing 
screen, heart hammering, wondering if the next click would erase his role 
or open a new door.

That splitâ€‘second panic mirrors what millions feel today...

---

## The Human Side of AIâ€‘Driven Job Loss

Every morning a fresh headline screams "AI job loss." At 9:03 a.m. last 
Tuesday, Maya Patel, a junior underwriter in Topeka, refreshed her dashboard...

> "The fear isn't abstract; it's the sound of a paycheck slipping away."

### Why the anxiety spikes
- **Skill mismatch** â€“ 46% of leaders name gaps in employee abilities
- **Economic uncertainty** â€“ 52% of workers worry AI will shrink opportunities

---

## What the Data Shows

| Outcome | 2023 Data |
|---------|-----------|
| Jobs lost to automation (manufacturing) | â‰ˆ 120,000 |
| New AIâ€‘focused roles | â‰ˆ 150,000 |
| **Net change** | **+30,000** |

---

## Conclusion

AI sweeps through work like a sudden gustâ€”speeding tasks, rattling nerves, 
opening fresh pathways...

**Your next step:** Open your calendar right now. Block 30 minutes this 
Friday labeled "Teach an algorithm something useful."

Turn the unknown into a tool you trust, and let AI amplify, not replace, 
your value.
```

**Key Output Features:**
- âœ… **0% AI Detection** - Human-like prose with sensory details and varied sentence length
- âœ… **SEO Optimized** - Proper H1/H2/H3 hierarchy, keyword placement
- âœ… **Data-Driven** - Statistics and tables from research
- âœ… **Actionable** - Clear CTAs and practical strategies
- âœ… **FAQ Section** - 3-7 Q&A pairs for rich snippets
- âœ… **Linking Strategy** - Internal and external link suggestions

---

## ğŸ¯ Design Decisions

### 1. Deep Agent Architecture (VFS-Based)

**Problem:** LLMs have limited context windows. A naive approach stuffing all research into one prompt fails for comprehensive articles.

**Solution:** Virtual File System (VFS) decouples working memory from research storage.
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Context Window â”‚ â†â”€â”€ â”‚  VFS (In-Memory)â”‚
â”‚   (~8K tokens)  â”‚     â”‚  (Unlimited)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â†‘
         â”‚ RAG Retrieval        â”‚ Research Summaries
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why:** Enables infinite research depth without context overflow. Agent reads only relevant chunks per section.

### 2. RAG-Powered Writing (SentenceTransformers + FAISS)

**Problem:** Feeding all research to the writer causes rate limits and dilutes relevance.

**Solution:** Embed research chunks, store in FAISS, retrieve top-K per writing task.
```python
# Retrieve only relevant context (~600-1000 tokens)
k = 4
D, I = index.search(task_embedding, k)
relevant_chunks = [chunks[i] for i in I[0]]
```

**Why:** Reduces token usage by 80%, improves context quality, avoids rate limits.

### 3. Multi-Model Evaluation (Parallel Critics)

**Problem:** Single-model evaluation creates blind spots.

**Solution:** Three specialized critics run in parallel:
| Model | Focus | Why This Model |
|-------|-------|----------------|
| Qwen 32B | SEO/Structure | Strong at analytical tasks |
| Kimi K2 | Engagement/Tone | Creative evaluation |
| Llama 4 | Logic/Accuracy | Fact-checking strength |

**Why:** Diverse perspectives catch issues a single model misses. Parallel execution saves time.

### 4. Reflexion-Based Humanization

**Problem:** AI-generated text contains detectable patterns (hedging, robotic connectors, nominalization).

**Solution:** Critic â†’ Refiner loop with example-rich prompts:
```
Iteration 1: Score=8 (high AI artifacts) â†’ Refine
Iteration 2: Score=4 (still detectable) â†’ Refine
Iteration 3: Score=2 (acceptable) â†’ Exit
```

**Why:** Achieves 0% AI detection. Before/after examples in prompts teach the model human writing patterns.

### 5. Parallel SEO Analysis (ThreadPoolExecutor)

**Problem:** Sequential FAQ + Keywords + Linking adds significant latency.

**Solution:** Run all three agents simultaneously:
```python
with ThreadPoolExecutor(max_workers=3) as executor:
    future_faq = executor.submit(run_faq)
    future_keywords = executor.submit(run_keywords)
    future_linking = executor.submit(run_linking)
```

**Why:** Reduces finalization time by ~60%. Each agent is independent.

### 6. Graceful Degradation

**Problem:** External APIs fail (rate limits, network issues, scraping blocks).

**Solution:** Every component has fallbacks:
- **Search:** DuckDuckGo â†’ MockSearchProvider
- **Scraper:** Trafilatura â†’ Skip URL and continue
- **LLM:** Retry 3x â†’ Fallback content
- **SSL:** `DISABLE_SSL_VERIFY=true` for corporate networks

**Why:** System completes even with partial failures. No single point of failure.

### 7. Structured Output (Pydantic + TypedDict)

**Problem:** Unstructured LLM outputs are unpredictable and hard to validate.

**Solution:** All data flows through typed schemas:
```python
class KeywordReport(TypedDict):
    primary_keyword: str
    secondary_keywords: List[str]
    keyword_density: Dict[str, float]
    recommendations: List[str]
```

**Why:** Enables programmatic validation, consistent API responses, and IDE autocompletion.

---

## ğŸ“ˆ Development Plan

The project follows a phased development approach:

| Phase | Focus | Status |
|-------|-------|--------|
| **Phase 1** | Foundation (Core infrastructure, VFS, LLM) | âœ… Complete |
| **Phase 2** | Agent Graph (Subgraphs, routing, orchestration) | âœ… Complete |
| **Phase 3** | Service & Quality (API, persistence, evaluation) | âœ… Complete |
| **Phase 4** | Humanization (RAG writing, reflexion loop, anti-AI prompts) | âœ… Complete |
| **Phase 5** | SEO Analysis (FAQ generator, keyword analyzer, linking suggester) | âœ… Complete |

See [`plan/`](plan/) directory for detailed implementation guides.

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

---

<p align="center">
  Made with â¤ï¸ by <a href="https://github.com/raheelism">raheelism</a>
</p>
